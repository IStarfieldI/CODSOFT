# -*- coding: utf-8 -*-
"""Titanic Survival Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mTsDLJZJSetMRxvmWPgxZXME7Sfoja3Z

# TITANIC CLASSIFICATION
 Algorithm which tells whether the person will be save from sinking or not
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/Titanic-Dataset.csv')

df

df.shape

df.info

df['Age'].mean()

def male_female_child(Passenger):
  age, sex = Passenger
  if age<16:
    return 'child'
  else:
      return sex

df['Person'] = df[['Age','Sex']].apply(male_female_child, axis=1)

female_count = (df['Sex']=='female').sum()
print(female_count)

survivors = (df['Survived']==1).sum()
print(survivors)

df[0:10]

df['Person'].value_counts()

df['Survived'].value_counts()

df['Embarked'].value_counts()

df['Pclass'].isnull()

df = df.rename(columns={'Embarked.' : 'Embarked'})

#Fillinh null values in age column with mean values of age coulmn
df['Age'].fillna(df['Age'].mean(), inplace=True)

#Filling null values in embarked column with mode values of embarked column
df['Embarked'].fillna(df['Embarked'].mode()[0],inplace=True)

df.isna().sum()

df[['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',
       'Parch', 'Ticket', 'Fare', 'Embarked']].nunique().sort_values()

unique_counts = df.nunique()
sorted_counts = unique_counts.sort_values()
print(unique_counts)
print(sorted_counts)

df['Survived'].unique()

df['Sex'].unique()

df['Pclass'].unique()

df['Pclass'].unique().tolist()

df.drop(columns=['PassengerId','Name','Ticket'], axis=1, inplace=True)

df.info()

df.describe()

df.columns.values

sns.boxplot(x='Survived', y='Fare', data=df)

sns.boxplot(df)

df[df['Fare']>300]

sns.countplot(x='Sex', data=df, palette='Set2',edgecolor=(0,0,0),linewidth=1)
plt.show()

sns.set(style='darkgrid')
sns.countplot(x='Survived',hue='Sex',data=df,palette='Set2',edgecolor=(0,0,0),linewidth=1)
plt.xlabel('Survived')
plt.ylabel('Count')
plt.title('Survival Count by Gender')

plt.legend(title='Sex',loc='upper right', labels=['Female','Male'])
plt.show()
plt.show()

df

sns.countplot(x='Embarked', hue='Sex', data=df, palette='Set2',edgecolor=(0,0,0), linewidth=1)

sns.catplot(x='Pclass', kind='count',hue='Sex',data=df,palette='Set2',edgecolor=(0,0,0), linewidth=1)

sns.kdeplot(x='Age', data=df)

print(df['Survived'].value_counts())
sns.countplot(x='Survived', data=df, palette='Set2',edgecolor=(0,0,0), linewidth=1)
plt.show()

sns.countplot(x='SibSp', hue='Survived', data=df, palette='Set2',edgecolor=(0,0,0), linewidth=1)

sns.kdeplot(x='Survived', hue= 'Age', data=df, palette='Set2')

fig = sns.FacetGrid(df, hue='Sex', aspect=4)
fig.map(sns.kdeplot,'Age',shade= True)

# Set the x max limit by the oldest passenger
oldest = df['Age'].max()

#Since we know no one can be negative years old set the x lower limit at 0
fig.set(xlim=(0,oldest))

#Finally add a legend
fig.add_legend()

df.corr()

sns.heatmap(df.corr(), annot=True, cmap='coolwarm')

sns.pairplot(df)

df['Survived'].value_counts()

sns.countplot(x='Survived', data=df, palette='Set2',edgecolor=(0,0,0), linewidth=1)

import sklearn
print(sklearn.__version__)

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()
for column in ['Sex','Embarked']:
  df[column] = le.fit_transform(df[column])
df.head(10)

df['Fare'] = df['Fare'].astype(int)

df['Age'] = df['Age'].astype(int)

df

df[df['Age']<10].groupby(['Sex','Pclass']).mean()

"""<b>Children below 18 years</b> of age have higher chances of surviving, proven they saved childen first"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score
from sklearn.neighbors import KNeighborsClassifier

cols=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']
x=df[cols]
y=df['Survived']
print(x.shape)
print(y.shape)
print(type(x))
print(type(y))

x.head(5)

y.head(5)

x_train,x_test, y_train,y_test=train_test_split(x,y,test_size=0.10,random_state=1)
print(x_train.shape,'\n',
      x_test.shape,'\n',
      y_train.shape,'\n',
      y_test.shape)

def cls_eval(ytest,ypred):
    cm = confusion_matrix(ytest,ypred)
    print('Confusion Matrix\n',cm)
    print('Classification Report\n',classification_report(ytest,ypred))

def mscore(model):
    print('Training Score',model.score(x_train,y_train))  # Training Accuracy
    print('Testing Score',model.score(x_test,y_test))



""" ## 1. Logistic Regression </li>"""

lr = LogisticRegression(max_iter=1000, solver='liblinear')
lr.fit(x_train, y_train)

ypred_lr = lr.predict(x_test)
print(ypred_lr)

mscore(lr)

accuracy = accuracy_score(y_test, ypred_lr)
print("Accuracy:", accuracy)

# Evaluate the model - confusion matrix, classification Report, Accuracy score
cls_eval(y_test,ypred_lr)
acc_lr = round(accuracy_score(y_test,ypred_lr) * 100 ,2)
print('Accuracy Score',acc_lr)

"""##  2. DecisionTree Classifier"""

dt = DecisionTreeClassifier(max_depth=5, criterion='entropy', min_samples_split=10)
dt.fit(x_train, y_train)

mscore(dt)

ypred_dt = dt.predict(x_test)
print(ypred_dt)

# Evaluate the model - confusion matrix, classification Report, Accuracy score
cls_eval(y_test,ypred_dt)
acc_dt = round(accuracy_score(y_test,ypred_dt) *100,2)
print('Accuracy Score',acc_dt)

"""## KNN Classifier"""

# Building the knnClassifier Model
knn=KNeighborsClassifier(n_neighbors=8)
knn.fit(x_train,y_train)

# Computing Training and Testing score
mscore(knn)

# Generating Prediction
ypred_knn = knn.predict(x_test)
print(ypred_knn)

# Evaluate the model - confusion matrix, classification Report, Accuracy score
cls_eval(y_test,ypred_knn)
acc_knn = round(accuracy_score(y_test,ypred_knn) *100,2)
print('Accuracy Score',acc_knn)

models = pd.DataFrame(
{
    'Model':['Logistic Regression','Decision Tree','knn'],
    'Score':[acc_lr, acc_dt,acc_knn]
})

models.sort_values(by= 'Score', ascending= False)

from sklearn.model_selection import cross_val_predict

predictions = cross_val_predict(dt, x_train, y_train, cv=3)
confusion_matrix(y_train, predictions)

colors = ["blue", "yellow","orange"]

sns.set_style("whitegrid")
plt.figure(figsize=(15,5))
plt.ylabel("Accuracy %")
plt.xlabel("Algorithms")
sns.barplot(x=models['Model'],y=models['Score'], palette=colors )
plt.show()

"""<b> DecisionTree Classifier Model got the Highest Accuracy"""

